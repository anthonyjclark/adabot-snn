{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator Neural Network\n",
    "\n",
    "1. Try other activation functions\n",
    "2. Try other optimizers\n",
    "3. Try different numbers of hidden nodes\n",
    "4. Try different number of hidden layers\n",
    "5. Try different learning rates\n",
    "6. Try different batch sizes\n",
    "7. Try different number of epochs\n",
    "8. Try different loss functions\n",
    "9. Try shuffle training dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        \n",
    "        # fc -> fully connected\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UGVDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.len = 6\n",
    "        self.xsize = 3\n",
    "        self.ysize = 1\n",
    "        \n",
    "        self.x = torch.randn(self.len, self.xsize)\n",
    "        self.y = torch.randn(self.len, self.ysize)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def input_size(self):\n",
    "        return self.xsize\n",
    "    \n",
    "    def output_size(self):\n",
    "        return self.ysize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "hidden_size = 5\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "\n",
    "train_pct = 0.8\n",
    "do_shuffle = True\n",
    "random_seed = 842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = UGVDataset()\n",
    "# unpickled_df = pd.read_pickle(\"./dummy.pkl\")\n",
    "\n",
    "# Create training and testing samplers\n",
    "dataset_size = len(dataset)\n",
    "all_indices = list(range(dataset_size))\n",
    "\n",
    "if do_shuffle:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(all_indices)\n",
    "\n",
    "split_index = int(np.floor(train_pct * dataset_size))\n",
    "\n",
    "train_indices = all_indices[:split_index]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "\n",
    "test_indices = all_indices[split_index:]\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = dataset.input_size()\n",
    "output_size = dataset.output_size()\n",
    "\n",
    "# Neural network\n",
    "model = SNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Optimization function (loss)\n",
    "# criterion = nn.CrossEntropyLoss() # <-- for classification\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimization algorithm\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [1/2], Loss: 0.1646\n",
      "Epoch: [1/5], Step: [2/2], Loss: 0.2274\n",
      "Epoch: [2/5], Step: [1/2], Loss: 0.2293\n",
      "Epoch: [2/5], Step: [2/2], Loss: 0.1596\n",
      "Epoch: [3/5], Step: [1/2], Loss: 0.2289\n",
      "Epoch: [3/5], Step: [2/2], Loss: 0.1576\n",
      "Epoch: [4/5], Step: [1/2], Loss: 0.1565\n",
      "Epoch: [4/5], Step: [2/2], Loss: 0.2285\n",
      "Epoch: [5/5], Step: [1/2], Loss: 0.2283\n",
      "Epoch: [5/5], Step: [2/2], Loss: 0.1532\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "num_steps = len(train_loader)\n",
    "print_step = 1\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "\n",
    "        # Move tensors to the configured device\n",
    "        x = x.reshape(-1, input_size).to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat = model(x)        \n",
    "        loss = criterion(y_hat, y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        i1 = i + 1\n",
    "        e1 = epoch + 1\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i1 % print_step == 0:\n",
    "            l = running_loss / print_step\n",
    "            running_loss = 0.0\n",
    "            print(f'Epoch: [{e1}/{num_epochs}], Step: [{i1}/{num_steps}], Loss: {l:.4f}')\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 0.9674165844917297\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "    \n",
    "        x = x.reshape(-1, input_size).to(device)\n",
    "        y = y.to(device)\n",
    "        y_hat = model(x)\n",
    "        total_loss += criterion(y_hat, y)\n",
    "\n",
    "    print('Total loss:', total_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
